## ğŸ§  Definice UmÄ›lÃ© Inteligence

> **AI = Studium toho, jak umoÅ¾nit strojÅ¯m jednat tak, aby jejich chovÃ¡nÃ­ mohlo bÃ½t povaÅ¾ovÃ¡no za inteligentnÃ­ (1995).**

---

### ğŸ’¡ Ale co je to inteligence?
- **1950 â€“ Alan Turing** navrhl **TuringÅ¯v test**, kterÃ½ zkoumÃ¡, zda lze rozpoznat, zda odpovÄ›di pochÃ¡zÃ­ od ÄlovÄ›ka nebo stroje.

---

## ğŸ¤– UmÄ›lÃ¡ inteligence

RozliÅ¡ujeme dva zÃ¡kladnÃ­ typy AI:  
- **Narrow (slabÃ¡, ÃºzkÃ¡) AI**  
- **General (silnÃ¡, obecnÃ¡) AI**

---

### 1ï¸âƒ£ Narrow AI  
*(Ãºzce zamÄ›Å™enÃ¡ umÄ›lÃ¡ inteligence)*  

1. ZamÄ›Å™ena na konkrÃ©tnÃ­ Ãºkoly nebo oblasti.  
2. ZÃ¡kladnÃ­m principem je **strojovÃ© uÄenÃ­** â€“ uÄenÃ­ se na zÃ¡kladÄ› dat.  
3. NemÃ¡ schopnost **generalizace** mimo definovanÃ½ rÃ¡mec.  
4. **PÅ™Ã­klady:** computer vision, LLMs, machine learning algoritmy, hlasovÃ­ asistenti.  
5. **VÃ½hody:**
   - Vysoce efektivnÃ­ pÅ™i Å™eÅ¡enÃ­ specifickÃ½ch problÃ©mÅ¯.  
   - BezpeÄnÄ›jÅ¡Ã­ neÅ¾ obecnÃ¡ AI â€“ nemÅ¯Å¾e pÅ™ekroÄit danÃ© hranice.  
6. **NevÃ½hody:**
   - Neschopnost uÄit se mimo Ãºzce definovanou oblast.  
   - ZÃ¡vislost na velkÃ©m mnoÅ¾stvÃ­ dat.  
   - OmezenÃ¡ flexibilita a nepochopenÃ­ Å¡irÅ¡Ã­ch souvislostÃ­.  

---

### 2ï¸âƒ£ General AI  
*(obecnÃ¡ umÄ›lÃ¡ inteligence)*  

1. CÃ­l: vytvoÅ™it systÃ©my schopnÃ© **obecnÃ© inteligence** â€“ uÄit se, myslet, uvaÅ¾ovat, rozhodovat se.  
2. UmÃ­ vykonÃ¡vat Å¡irokou Å¡kÃ¡lu ÃºkolÅ¯, Å™eÅ¡it neznÃ¡mÃ© problÃ©my a **generalizovat** znalosti.  
3. DokÃ¡Å¾e napodobit, nebo dokonce **pÅ™ekonat ÄlovÄ›ka**.  
4. **VÃ½hody:**
   - PÅ™izpÅ¯sobivost novÃ½m ÃºkolÅ¯m bez nutnosti pÅ™eprogramovÃ¡nÃ­.  
5. **NevÃ½hody:**
   - ExtrÃ©mnÃ­ technickÃ¡ nÃ¡roÄnost.  
   - SouÄasnÃ¡ (slabÃ¡) AI je zatÃ­m velmi vzdÃ¡lenÃ¡ teoretickÃ©mu cÃ­li GAI.  
   - ZÃ¡vaÅ¾nÃ© **etickÃ© otÃ¡zky**.  

---

## ğŸ” StrojovÃ© uÄenÃ­ vs. UmÄ›lÃ¡ inteligence

- VeÅ¡kerÃ¡ AI vyuÅ¾Ã­vÃ¡ **proces uÄenÃ­ na datech**.  
- **StrojovÃ© uÄenÃ­ (Machine Learning)** je **podmnoÅ¾inou AI**.  

---

## ğŸ“˜ Typy uÄenÃ­

## 1. UÄenÃ­ s uÄitelem *(Supervised Learning)*  
- SystÃ©m se uÄÃ­ z **dat oznaÄenÃ½ch Å¡tÃ­tky (labely)**.  
- TrÃ©novacÃ­ sada obsahuje dvojice **[datovÃ½ vektor, label]**.  
- Label udÃ¡vÃ¡ sprÃ¡vnÃ© Å™eÅ¡enÃ­, kterÃ© systÃ©m napodobuje.  
- Po natrÃ©novÃ¡nÃ­ systÃ©m dostÃ¡vÃ¡ pouze **data** a na jejich zÃ¡kladÄ› **predikuje vÃ½sledek**. 
- lineÃ¡rnÃ­ regrese
- **PÅ™esnost modelu** se vyhodnocuje takto:
  1. Data se **promÃ­chajÃ­** (aby se odstranila systematickÃ¡ chyba).  
  2. RozdÄ›lÃ­ se na **trÃ©novacÃ­** a **testovacÃ­** sadu (napÅ™. 9:1 nebo 8:2).  
  3. Model se natrÃ©nuje na trÃ©novacÃ­ sadÄ› a **ovÄ›Å™Ã­** na testovacÃ­.  
  4. PorovnÃ¡nÃ­ vÃ½sledkÅ¯ s testovacÃ­mi daty urÄÃ­ **pÅ™esnost modelu**.  
  5. PorovnÃ¡nÃ­ s trÃ©novacÃ­mi daty je **neobjektivnÃ­** â€“ model by je uÅ¾ â€znalâ€œ.  
- regrese, klasifikace
- Regrese = lineÃ¡rnÃ­ fit
- Klasifikace:
### Klasifikace: k-NN (k-Nearest Neighbors)

**k-nejbliÅ¾Å¡Ã­ch sousedÅ¯** je jeden z nejjednoduÅ¡Å¡Ã­ch, ale velmi ÃºÄinnÃ½ch algoritmÅ¯ strojovÃ©ho uÄenÃ­.

#### Princip fungovÃ¡nÃ­

- **UÄenÃ­ s uÄitelem (Supervised Learning):** Algoritmus pracuje s daty, kterÃ¡ jiÅ¾ majÃ­ pÅ™iÅ™azenÃ© sprÃ¡vnÃ© labely (tÅ™Ã­dy).
    
- **LÃ­nÃ½ algoritmus (Lazy Learning):** Na rozdÃ­l od jinÃ½ch modelÅ¯ netvoÅ™Ã­ bÄ›hem trÃ©novÃ¡nÃ­ Å¾Ã¡dnou matematickou funkci. Pouze si â€zapamatujeâ€œ vÅ¡echna trÃ©ninkovÃ¡ data a veÅ¡kerÃ© vÃ½poÄty provÃ¡dÃ­ aÅ¾ ve chvÃ­li, kdy dostane novÃ½ bod ke klasifikaci.
    
- **Logika rozhodovÃ¡nÃ­:** NovÃ½ datovÃ½ bod je vloÅ¾en do prostoru a algoritmus vyhledÃ¡ $k$ nejbliÅ¾Å¡Ã­ch sousedÅ¯. Bod je pak pÅ™iÅ™azen do tÃ© tÅ™Ã­dy, kterÃ¡ mÃ¡ mezi tÄ›mito sousedy vÄ›tÅ¡inu.
    

---

#### KlÃ­ÄovÃ© parametry

##### 1. Parametr $k$

UrÄuje, kolik sousedÅ¯ ovlivÅˆuje vÃ½sledek. VÃ½bÄ›r sprÃ¡vnÃ©ho $k$ je kritickÃ½ pro pÅ™esnost modelu:

- **MalÃ© $k$ (napÅ™. 1):** Model je velmi citlivÃ½ na Å¡um v datech. HrozÃ­ **overfitting** (pÅ™euÄenÃ­).
    
- **VelkÃ© $k$:** Model se stÃ¡vÃ¡ pÅ™Ã­liÅ¡ robustnÃ­m a ignoruje lokÃ¡lnÃ­ vzorce. HrozÃ­ **underfitting** (nedostateÄnÃ© nauÄenÃ­).
    
- **Tip:** Obvykle se volÃ­ **lichÃ© ÄÃ­slo**, aby se pÅ™edeÅ¡lo remÃ­ze pÅ™i hlasovÃ¡nÃ­ o tÅ™Ã­dÄ›.
    

##### 2. Metrika vzdÃ¡lenosti

UrÄuje, jakÃ½m zpÅ¯sobem mÄ›Å™Ã­me â€blÃ­zkostâ€œ bodÅ¯ v prostoru.

- **EukleidovskÃ¡ vzdÃ¡lenost:** NejpouÅ¾Ã­vanÄ›jÅ¡Ã­ (pÅ™Ã­mÃ¡ ÄÃ¡ra mezi body).
    
- **ManhattanskÃ¡ vzdÃ¡lenost:** SouÄet absolutnÃ­ch rozdÃ­lÅ¯ souÅ™adnic.

---

## 2. UÄenÃ­ bez uÄitele (Unsupervised Learning)

V tomto reÅ¾imu algoritmus pracuje s daty, kterÃ¡ **neobsahujÃ­ Å¾Ã¡dnÃ© Å¡tÃ­tky** (labely) ani â€sprÃ¡vnÃ¡ Å™eÅ¡enÃ­â€œ. Model se snaÅ¾Ã­ sÃ¡m najÃ­t skrytÃ© struktury, vzorce nebo podobnosti.

- **CÃ­l:** Nejde o predikci konkrÃ©tnÃ­ hodnoty, ale o **pochopenÃ­ vnitÅ™nÃ­ struktury dat**.
    
- **Analogie:** PÅ™edstavte si, Å¾e dostanete krabici plnou rÅ¯znÃ½ch mincÃ­ z celÃ©ho svÄ›ta, aniÅ¾ byste vÄ›dÄ›li, co jsou zaÄ. VaÅ¡Ã­m Ãºkolem je roztÅ™Ã­dit je podle velikosti, barvy nebo materiÃ¡lu.

### HlavnÃ­ Ãºlohy a metody

#### 1. ShlukovÃ¡nÃ­ (Clustering)

RozdÄ›luje data do skupin (shlukÅ¯) tak, aby si objekty uvnitÅ™ jednÃ© skupiny byly co nejvÃ­ce podobnÃ© a objekty z rÅ¯znÃ½ch skupin co nejvÃ­ce odliÅ¡nÃ©.

- **PÅ™Ã­klady:** Segmentace zÃ¡kaznÃ­kÅ¯ (marketing), seskupovÃ¡nÃ­ podobnÃ½ch zprÃ¡v v Google News, analÃ½za genÅ¯.
    
- **KlÃ­ÄovÃ© algoritmy:** * **K-Means:** RozdÄ›luje data do pÅ™edem urÄenÃ©ho poÄtu $k$ shlukÅ¯.
    - **DBSCAN:** HledÃ¡ shluky na zÃ¡kladÄ› hustoty bodÅ¯ (skvÄ›lÃ© pro nepravidelnÃ© tvary).
    - **HierarchickÃ© shlukovÃ¡nÃ­:** VytvÃ¡Å™Ã­ stromovou strukturu (dendrogram) vztahÅ¯.
    
```
		### **2ShlukovÃ¡nÃ­ (K-Means)**

Metoda **K-Means** je nejpouÅ¾Ã­vanÄ›jÅ¡Ã­m algoritmem pro shlukovÃ¡nÃ­ dat, u kterÃ½ch pÅ™edem neznÃ¡me sprÃ¡vnÃ© Å™eÅ¡enÃ­ (nemÃ¡me labely).

#### **ZÃ¡kladnÃ­ princip**

- Algoritmus rozdÄ›luje data do **$K$ shlukÅ¯** (skupin).
    
- CÃ­lem je, aby body uvnitÅ™ jednoho shluku byly co nejblÃ­Å¾e svÃ©mu stÅ™edu (**centroidu**) a zÃ¡roveÅˆ co nejdÃ¡le od bodÅ¯ v jinÃ½ch shlucÃ­ch.
    

#### **Algoritmus v 5 krocÃ­ch**

1. **Volba $K$:** UrÄÃ­me poÄet shlukÅ¯ (napÅ™. chci rozdÄ›lit zÃ¡kaznÃ­ky do 3 skupin).
    
2. **Inicializace:** Algoritmus nÃ¡hodnÄ› umÃ­stÃ­ $K$ centroidÅ¯ do prostoru mezi data.
    
3. **PÅ™iÅ™azenÃ­:** KaÅ¾dÃ½ datovÃ½ bod se pÅ™iÅ™adÃ­ k nejbliÅ¾Å¡Ã­mu centroidu.
    
4. **Aktualizace:** Centroidy se pÅ™esunou do skuteÄnÃ©ho geometrickÃ©ho stÅ™edu svÃ½ch novÃ½ch skupin.
    
5. **OpakovÃ¡nÃ­:** Kroky 3 a 4 se opakujÃ­, dokud se pozice centroidÅ¯ neustÃ¡lÃ­.
    

#### **PÅ™Ã­klad: Segmentace zÃ¡kaznÃ­kÅ¯**

- **Vstup:** Data o ÃºtratÄ› a vÄ›ku 1000 lidÃ­ (bez informace, kdo je kdo).
    
- **Proces:** K-Means najde shluky (napÅ™. "mladÃ­ s vysokou Ãºtratou", "senioÅ™i s nÃ­zkou Ãºtratou").
    
- **VÃ½stup:** Algoritmus body jen oÄÃ­sloje (Shluk 0, Shluk 1). **Interpretace a pojmenovÃ¡nÃ­** skupin je na ÄlovÄ›ku.
``` 

#### 2. Redukce dimenzionality (SnÃ­Å¾enÃ­ dimenze)

Proces zjednoduÅ¡enÃ­ dat s velkÃ½m poÄtem vlastnostÃ­ (sloupcÅ¯) pÅ™i zachovÃ¡nÃ­ co nejvÃ­ce dÅ¯leÅ¾itÃ½ch informacÃ­.
- **VyuÅ¾itÃ­:** Vizualizace sloÅ¾itÃ½ch (vÃ­cerozmÄ›rnÃ½ch) dat, odstranÄ›nÃ­ Å¡umu a zrychlenÃ­ dalÅ¡Ã­ch vÃ½poÄtÅ¯.
- **HlavnÃ­ metoda:** **PCA (Principal Component Analysis)** â€“ analÃ½za hlavnÃ­ch komponent.
    

#### 3. AsociaÄnÃ­ pravidla

HledÃ¡nÃ­ zajÃ­mavÃ½ch souvislostÃ­ a korelacÃ­ mezi promÄ›nnÃ½mi ve velkÃ½ch databÃ¡zÃ­ch.
- **Princip:** â€Pokud nastane jev A, s velkou pravdÄ›podobnostÃ­ nastane i jev B.â€œ
- **VyuÅ¾itÃ­:** AnalÃ½za nÃ¡kupnÃ­ho koÅ¡Ã­ku (Cross-selling). KlasickÃ½ (moÅ¾nÃ¡ i mÄ›stskÃ¡ legenda) pÅ™Ã­klad: Pivo a plenky â€“ lidÃ© kupujÃ­cÃ­ plenky v pÃ¡tek veÄer Äasto pÅ™ihodÃ­ i pivo.

---

## 3. UÄenÃ­ posilovÃ¡nÃ­m *(Reinforcement Learning)*  
- zpÄ›tnovazebnÃ© uÄenÃ­
- NemÃ¡me informaci o jednom datovÃ©m bodu zda je sprÃ¡vnÃ½, ale mÃ¡me informaci o shluku vÃ­ce datovÃ½ch bodÅ¯.
- NejdÅ™Ã­ve musÃ­te nÄ›co udÄ›lat, a pak se dozvÃ­te zda je to dobrÃ©. Metoda pokus omyl. SpecifickÃ© algoritmy se pouze snaÅ¾Ã­ optimalizovat metody odhadu, abychom k vÃ½sledku doÅ¡li nejrychleji, a vÃ½poÄetnÄ› nejlevnÄ›ji.
- pÅ™Ã­klad: PiÅ¡kworky. NedokÃ¡Å¾eme vyhodnotit, zda je jednotlivÃ½ specifickÃ½ tah sprÃ¡vnÃ½ (nenÃ­ uÄitel), ale po urÄitÃ© sekvenci tahÅ¯, jsme schopni vyhodnotit, zda tato posloupnost tahÅ¯ vedla k vÃ½hÅ™e Äi ne. Tj. zpÄ›tnovazebnÃ© uÄenÃ­.
- UÄenÃ­ probÃ­hÃ¡ pomocÃ­ rÅ¯znÃ½ch algoritmÅ¯
	- napÅ™. GenetickÃ© algoritmy, kdy simulujeme evoluci datovÃ½ch shlukÅ¯, kdyÅ¾ je nechÃ¡me mezi sebou nÃ¡hodnÄ› kÅ™Ã­Å¾it, kaÅ¾dÃ©mu datovÃ©mu shluku pÅ™iÅ™azujeme skÃ³re, a v dalÅ¡Ã­ch generacÃ­ kÅ™Ã­Å¾Ã­me pouze shluky s nejvÄ›tÅ¡Ã­m skÃ³rem, tj. princip evoluce. 
	- Touto metodou slepÄ› hledÃ¡me Å™eÅ¡enÃ­, bez znalosti skrytÃ© dynamiky systÃ©mu.
- DalÅ¡Ã­ metody Å™eÅ¡enÃ­ tÄ›chto problÃ©mÅ¯
	- DijkstrÅ¯v algoritmus - Å™eÅ¡Ã­ problÃ©m hledÃ¡nÃ­ cesty na specifickÃ©m grafu, kterÃ½ vypadÃ¡ jako mapa ÄtvereÄkÅ¯ se zaÄÃ¡tkem a cÃ­lem. KaÅ¾dÃ½ ÄtvereÄek mÃ¡ svou hodnotu = "cenu", kterÃ¡ mÅ¯Å¾e napÅ™. urÄovat nÃ¡roÄnost cesty danÃ½m smÄ›rem.
	- A* algoritmus - Odhaduje. VyuÅ¾Ã­vÃ¡ heurestiku. NapÅ™. chess engine, tj. pÅ™i hledÃ¡nÃ­ v grafu odhaduje hodnotu jednotlivÃ½ch pozic pomocÃ­ vzduÅ¡nÃ½ch Äar. PodobnÃ© jako DijkstrÅ¯v algoritmus
- ProhledÃ¡vÃ¡nÃ­ do Å¡Ã­Å™ky, do hloubky
- MnohÃ© dalÅ¡Ã­